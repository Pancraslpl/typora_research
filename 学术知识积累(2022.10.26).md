#                  一、Python相关知识

## 1.1 Python程序问题集锦

### （1）**Python中消除futureWarning问题**

```python
from warnings import simplefilter
simplefilter(action='ignore', category=FutureWarning)
```

### (2)回调函数

![image-20220924151814307](C:\Users\lpl\AppData\Roaming\Typora\typora-user-images\image-20220924151814307.png)

### （3）python中的format函数

1. **填充**

   通过位置来填充字符串

   ```python
   print('he1lo {0} i am {1}'.format('Kevin','Tom'))
    
   print('hello {} i am {}'.format('Kevin','Tom'))
    
   print('hello {0} i am {1} . my name is {0}'.format('Kevin','Tom'))
    
   #输出
   he1lo Kevin i am Tom
   hello Kevin i am Tom
   hello Kevin i am Tom . my name is Kevin
   ```

### （4）python中的class类及相关的_ _init_ _(self,  parameter1, parameter2) 函数

![image-20220926094333119](C:\Users\lpl\AppData\Roaming\Typora\typora-user-images\image-20220926094333119.png)

![image-20220926094405381](C:\Users\lpl\AppData\Roaming\Typora\typora-user-images\image-20220926094405381.png)

## 1.2 Python库函数

### （1）PrettyTable介绍与基本使用

相信很多小伙伴在使用`python`需要查看表格数据，直接`print`出来呢？又乱了。`PrettyTable`可以解决这个问题，说个简单的应用，在爬`12306`网站的数据，需要表格展示，更加清晰，如下图：
![在这里插入图片描述](https://img-blog.csdnimg.cn/9b354cf2c69b47f78507a82c025e6222.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAY3VudG91MDkwNg==,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center)
   这样在输出的窗口可以很清晰看到所需要的信息。那么类似这种表格要怎么做出来呢？没错，使用`PrettyTable`就会把事情变得很简单。

#### PrettyTable 安装

   `PrettyTable `安装很简单，直接使用`pip`安装即可：

```python
python -m pip install -U prettytable
####      或者
pip install -U prettytable
123
```

#### PrettyTable 基本使用

   `PrettyTable`用于创建和展示表格数据，所以就先创建个表格把。

**创建表格**

```python
from prettytable import PrettyTable
x = PrettyTable()
12
```

   当然我们可以设置表格的名称：

```python
x.title = 'Table 1  City Info'
1
```

**添加一行数据**

   接下来给表格添加数据，包括字段名和数据，分别用`field_name`s和`add_row`函数，例子来自官网：

```python
x.field_names = ["City name", "Area", "Population", "Annual Rainfall"]
x.add_row(["Adelaide", 1295, 1158259, 600.5])
x.add_row(["Brisbane", 5905, 1857594, 1146.4])
x.add_row(["Darwin", 112, 120900, 1714.7])
x.add_row(["Hobart", 1357, 205556, 619.5])
x.add_row(["Sydney", 2058, 4336374, 1214.8])
x.add_row(["Melbourne", 1566, 3806092, 646.9])
x.add_row(["Perth", 5386, 1554769, 869.4])
12345678
```

   现在就打印出来看看，直接`print(x)`。
![在这里插入图片描述](https://img-blog.csdnimg.cn/3fcf0100561a4bf3a16ec81af4232227.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAY3VudG91MDkwNg==,size_12,color_FFFFFF,t_70,g_se,x_16#pic_center)
   这种可视化的效果真的好。那接下来在看看其他的方法。

**一次性添加多行**

```python
x.field_names = ["City name", "Area", "Population", "Annual Rainfall"]
x.add_rows(
    [
        ["Adelaide", 1295, 1158259, 600.5],
        ["Brisbane", 5905, 1857594, 1146.4],
        ["Darwin", 112, 120900, 1714.7],
        ["Hobart", 1357, 205556, 619.5],
        ["Sydney", 2058, 4336374, 1214.8],
        ["Melbourne", 1566, 3806092, 646.9],
        ["Perth", 5386, 1554769, 869.4],
    ]
)
123456789101112
```

**添加一列**

```python
x.add_column("City name",
["Adelaide","Brisbane","Darwin","Hobart","Sydney","Melbourne","Perth"])
x.add_column("Area", [1295, 5905, 112, 1357, 2058, 1566, 5386])
x.add_column("Population", [1158259, 1857594, 120900, 205556, 4336374, 3806092,
1554769])
x.add_column("Annual Rainfall",[600.5, 1146.4, 1714.7, 619.5, 1214.8, 646.9,
869.4])
1234567
```

   注意：add_column的第一个参数表示列的字段名，为字符串，第二个参数为列表，即添加到该列的数据。没有一次性添加多列的方法。
  当然，可以混合使用`add_row`和`add_column`，但是混合使用很容易混乱。

**根据csv导入**
   `prettytable`支持从`csv`文件中导入数据并创建表格，需要注意的是，字符串需要加上引号。

```python
from prettytable import from_csv
with open("CityInfo.csv") as fp:
    mytable = from_csv(fp)
print(mytable)
1234
```

![在这里插入图片描述](https://img-blog.csdnimg.cn/ee28b47779054471b16f91443c70d6e2.png#pic_center)![在这里插入图片描述](https://img-blog.csdnimg.cn/1a57f74e14884f3cb82f67510148919d.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAY3VudG91MDkwNg==,size_12,color_FFFFFF,t_70,g_se,x_16#pic_center)
**从数据库中导入**

```python
import sqlite3
from prettytable import from_db_cursor

connection = sqlite3.connect("mydb.db")
cursor = connection.cursor()
cursor.execute("SELECT field1, field2, field3 FROM my_table")
mytable = from_db_cursor(cursor)
1234567
```

**表数据的删除**

   `prettytable`提供四种方法用于删除数据：

- `del_row`：删除某行，允许传入一个整数参数，（从0开始）。
- `del_column`：删除某列，允许传入一个字符串，表示要删除的列的字段名。
- `clear_rows`：删除所有数据，但保存列的字段名。
- `clear`：删除所有数据，包括列的字段名。

**显示表格**
   前面弄了那么多，都是为了最后的表格显示，这里说说`prettytable`的显示表格方法。最简单的就是：

```python
print(x)
1
mystring = x.get_string()
1
```

   `get_string()`函数可以将上面`print`的结果直接转化为字符串，可以将这个结果写到文本文件里。当然也可以打印出来。

```python
print(x.get_string())
1
```

**显示指定的列**

```python
print(x.get_string(fields=["City name", "Population"]))
1
```

![在这里插入图片描述](https://img-blog.csdnimg.cn/cb8ccf09e6754f1081bab73f8e6ff797.png#pic_center)

**显示指定的行**

```python
print(x.get_string(start=1, end=4))
1
```

  这里的`start`和`end`分别表示所要显示的起始行和终止行。这里的索引和`python`一样，从`0`开始，并且不包括`end`所指的行。
![在这里插入图片描述](https://img-blog.csdnimg.cn/ad0c566caf0b4081beb63a2d4de32aa9.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAY3VudG91MDkwNg==,size_13,color_FFFFFF,t_70,g_se,x_16#pic_center)二、机器学习与深度学习相关知识

### （2）用scipy库中的io.loadmat读取.mat文件

```
from scipy import io
data = io.loadmat(os.path.join(folder, domain + ‘’))
```

### （3）cvxopt库

```
1. 创建矩阵

2. 求解线性规划
from cvxopt import matrix, solvers
A = matrix([ [-1.0, -1.0, 0.0, 1.0], [1.0, -1.0, -1.0, -2.0] ])
b = matrix([ 1.0, -2.0, 0.0, 4.0 ])
c = matrix([ 2.0, 1.0 ])
sol=solvers.lp(c,A,b)
pcost dcost gap pres   dres   k/t
0:  2.6471e+00 -7.0588e-01  2e+01  8e-01  2e+00  1e+00
1:  3.0726e+00  2.8437e+00  1e+00  1e-01  2e-01  3e-01
2:  2.4891e+00  2.4808e+00  1e-01  1e-02  2e-02  5e-02
3:  2.4999e+00  2.4998e+00  1e-03  1e-04  2e-04  5e-04
4:  2.5000e+00  2.5000e+00  1e-05  1e-06  2e-06  5e-06
5:  2.5000e+00  2.5000e+00  1e-07  1e-08  2e-08  5e-08
>>> print(sol['x'])
[ 5.00e-01]
[ 1.50e+00]
```

### （4）pylustrator

只需要导入pylustrator，开启pylustrator即可，添加两行代码：

```text
# 导入pylustrator 
import pylustrator 

# 开启pylustrator 
pylustrator.start()
```

举个例子，

```text
import numpy as np
import matplotlib.pyplot as plt 

import pylustrator  
pylustrator.start() #开启pylustrator


def f(t):
    return np.exp(-t) * np.cos(2 * np.pi * t)


t1 = np.arange(0.0, 5.0, 0.1)
t2 = np.arange(0.0, 5.0, 0.02)

plt.style.use('ggplot')
plt.subplot(211)
plt.plot(t1, f(t1), color='tab:blue', marker='o')
plt.plot(t2, f(t2), color='black')

plt.subplot(212)
plt.plot(t2, np.cos(2 * np.pi * t2), color='tab:orange', linestyle='--')
plt.show()
```



# 二、 机器学习与深度学习相关知识

## （1）欠采样和过采样

过采样和欠采样是处理非平衡分类问题时的常用手段。

拿二元分类为例，如果训练集中阳性样本有1000个，阴性样本有10万个，两者比例为1：100严重失衡。为了一些模型的性能考虑，我们需要进行一些处理使得两者的比例尽可能接近。

过采样：对少的一类进行重复选择，比如我们对1000个阳性样本进行有放回的抽样，抽5万次（当然其中有很多重复的样本），现在两类的比例就变成了1：2，比较平衡。

欠采样：对多的一类进行少量随机选择，比如我们对10万个阴性样本进行随机选择，抽中2000个（当然原样本中很多样本未被选中），现在两类的比例就变成了1：2，比较平衡。

SMOTE：SMOTE算法的基本思想就是对少数类别样本进行分析和模拟，并将人工模拟的新样本添加到数据集中，进而使原始数据中的类别不再严重失衡。该算法的模拟过程采用了KNN技术。

## （2）经验风险最小化与结构风险最小化

#### 经验风险最小化

经验风险最小化的策略认为，经验风险最小的模型是最优的模型：

![[公式]](https://www.zhihu.com/equation?tex=+%5Cmin_%7Bf%5Cin+F%7D%5Cfrac%7B1%7D%7BN%7D%5Csum%5EN_%7Bi%3D1%7DL%28y_i%2Cf%28x_i%29%29%5Ctag7+%5C%5C)

当样本**容量足够大**时，经验风险最小化能保证有很好的学习效果。比如，**极大似然估计**就是经验风险最小化的一个例子，`当模型是条件概率分布，损失函数是对数损失函数时，经验风险最小化就等价于极大似然估计`。

但当样本容量很小时，经验风险最小化容易导致“过拟合”。

#### 结构风险最小化

结构风险最小化（structural minimization, SRM）是为了防止过拟合提出的策略。结构风险最小化等价于正则化（regularization）。结构风险在经验风险上加上表示模型复杂度的**正则化项**（regularizer）或**罚项**（penalty term）。结构风险的定义是：

![[公式]](https://www.zhihu.com/equation?tex=+R_%7Bsrm%7D%28f%29%3D%5Cfrac%7B1%7D%7BN%7D%5Csum%5E%7BN%7D_%7Bi%3D1%7DL%28y_i%2Cf%28x_i%29%29%2B%5Clambda+J%28f%29+%5Ctag8+%5C%5C)

其中![[公式]](https://www.zhihu.com/equation?tex=J%28f%29)是模型复杂度的函数，![[公式]](https://www.zhihu.com/equation?tex=%5Clambda%5Cgeq0)是系数，用来权衡经验风险和模型复杂度。

结构风险最小化的策略认为结构风险最小的模型是最优模型：

![[公式]](https://www.zhihu.com/equation?tex=+%5Cmin_%7Bf%5Cin+F%7D%5B%5Cfrac%7B1%7D%7BN%7D%5Csum%5EN_%7Bi%3D1%7DL%28y_i%2Cf%28x_i%29%29%2B%5Clambda+J%28f%29%5D+%5C%5C)

结构风险小`需要经验风险和模型复杂度同时都小`，结构风险小的模型往往对训练数据以及未知的测试数据都有较好的预测。

比如，贝叶斯估计中的最大后验概率估计（maximum posterior probability estimation,MAP）就是结构风险最小化的一个例子，`当模型是条件概率分布、损失函数是对数损失函数、模型复杂度由模型的先验概率表示时`，结构风险最小化就等价于最大后验概率估计。

## **（3）最小二乘法的几何意义**

![image-20220913185923965](C:\Users\lpl\AppData\Roaming\Typora\typora-user-images\image-20220913185923965.png)

## （4）极大似然估计

极大似然估计是求取似然函数的概率最大时的参数值

<img src="C:\Users\lpl\AppData\Roaming\Typora\typora-user-images\image-20220915212017806.png" alt="image-20220915212017806" style="zoom:50%;" />

##  (5) **Python sklearn机器学习的分类和回归评价指标——Sklearn.metrics简介及应用示例**

 无论利用机器学习算法进行回归、分类或者聚类时，**评价指标**，即检验机器学习模型效果的定量指标，都是一个不可避免且十分重要的问题。

例如：

```python
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score
```

![点击并拖拽以移动](data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==)调用方式为：直接使用函数名调用，计算均方误差mean squared error

mse = mean_squared_error(y_test, y_pre)

 计算回归的决定系数R2：

R2 = r2_score(y_test,y_pre)

**回归指标**

```python
explained_variance_score(y_true, y_pred, sample_weight=None, multioutput=‘uniform_average’)：回归方差(反应自变量与因变量之间的相关程度)

mean_absolute_error(y_true,y_pred,sample_weight=None,
multioutput=‘uniform_average’)：
平均绝对误差

mean_squared_error(y_true, y_pred, sample_weight=None, multioutput=‘uniform_average’)：均方差

median_absolute_error(y_true, y_pred) 中值绝对误差

r2_score(y_true, y_pred,sample_weight=None,multioutput=‘uniform_average’) ：R平方值
```

**分类指标**

```python
accuracy_score(y_true,y_pre) : 精度

auc(x, y, reorder=False) : ROC曲线下的面积;较大的AUC代表了较好的performance。

average_precision_score(y_true, y_score, average=‘macro’, sample_weight=None):根据预测得分计算平均精度(AP)

brier_score_loss(y_true, y_prob, sample_weight=None, pos_label=None):The smaller the Brier score, the better.

confusion_matrix(y_true, y_pred, labels=None, sample_weight=None):通过计算混淆矩阵来评估分类的准确性 返回混淆矩阵

f1_score(y_true, y_pred, labels=None, pos_label=1, average=‘binary’, sample_weight=None): F1值
　　F1 = 2 * (precision * recall) / (precision + recall) precision(查准率)=TP/(TP+FP) recall(查全率)=TP/(TP+FN)

log_loss(y_true, y_pred, eps=1e-15, normalize=True, sample_weight=None, labels=None)：对数损耗，又称逻辑损耗或交叉熵损耗

precision_score(y_true, y_pred, labels=None, pos_label=1, average=‘binary’,) ：查准率或者精度； precision(查准率)=TP/(TP+FP)

recall_score(y_true, y_pred, labels=None, pos_label=1, average=‘binary’, sample_weight=None)：查全率 ；recall(查全率)=TP/(TP+FN)

roc_auc_score(y_true, y_score, average=‘macro’, sample_weight=None)：计算ROC曲线下的面积就是AUC的值，the larger the better

roc_curve(y_true, y_score, pos_label=None, sample_weight=None, drop_intermediate=True)；计算ROC曲线的横纵坐标值，TPR，FPR
　　TPR = TP/(TP+FN) = recall(真正例率，敏感度) FPR = FP/(FP+TN)(假正例率，1-特异性)
```

## （6）**关于分类中的混淆矩阵、ROC(AUC)曲线的解释**

**混淆矩阵（confusion matrix）是机器学习中用来总结分类模型预测结果的一个分析表，它以矩阵的形式描绘样本数据的真实属性和分类预测结果类型之间的关系。**

**模型分类的效果如何，就借助一些专门的评价指标。可用于评估分类模型预测效果的指标有准确率（accuracy）、精确率（precision）、召回率（recall）和F1值。**

 <img src="https://img-blog.csdnimg.cn/d80de1d1d0da4fa39b271b3a212a5796.jpg?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA6L2p6KW_6ZOO,size_20,color_FFFFFF,t_70,g_se,x_16" alt="img" style="zoom: 33%;" />![点击并拖拽以移动](data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==)

<img src="https://img-blog.csdnimg.cn/3159441841fc428e869efa0d5023d233.jpg?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA6L2p6KW_6ZOO,size_20,color_FFFFFF,t_70,g_se,x_16" alt="img" style="zoom:33%;" />![点击并拖拽以移动](data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==)

混淆矩阵中，1）准确率（accuracy）是，被分对的样本数除以所有的样本数，对于分类均衡数据使用；2）准确度，表示被分为正例的示例中实际为正例的比例；3）召回率(recall)，召回率也叫查全率是覆盖面的度量，度量有多少个正例被分为正例。4）精度(precision)：反映测量结果与真值接近程度的量，它与误差的大小相对应，因此可用误差大小来表示精度的高低，误差小则精度高，误差大则精度低。5）F1-score如下：
$$
F1-score=2*[(precision*recall)/(precision+recall)]
$$

$$
accuracy=(TP+TN)/(TP+TN+FP+FN)
$$

**ROC曲线绘制的Python实现**

熟悉sklearn的读者肯定都知道，**几乎所有评估模型的指标都来自sklearn库下面的metrics**，包括计算召回率，精确率等。ROC曲线的绘制也不例外，都得先计算出评估的指标，也就是从metrics里面去调用roc_curve, auc，然后再去绘制。

```python
from sklearn.metrics import roc_curve, auc
```

roc_curve和auc的官方说明教程示例如下:

```python
# 数据准备
>>> import numpy as np
>>> from sklearn import metrics
>>> y = np.array([1, 1, 2, 2])
>>> scores = np.array([0.1, 0.4, 0.35, 0.8])

# roc_curve的输入为
# y: 样本标签
# scores: 模型对样本属于正例的概率输出
# pos_label: 标记为正例的标签，本例中标记为2的即为正例
>>> fpr, tpr, thresholds = metrics.roc_curve(y, scores, pos_label=2)
# 假阳性率
>>> fpr
array([ 0. ,  0.5,  0.5,  1. ])
# 真阳性率
>>> tpr
array([ 0.5,  0.5,  1. ,  1. ])
# 阈值
>>> thresholds
array([ 0.8 ,  0.4 ,  0.35,  0.1 ])

# auc的输入为很简单，就是fpr, tpr值
>>> auc = metrics.auc(fpr, tpr)
>>> auc
0.75

```

## （7）epoch和iteration的关系

iteration：1个iteration等于使用batchsize个样本训练一次；
epoch：1个epoch等于使用训练集中的全部样本训练一次；

比如要做100次iteration才等于做1次epoch训练。

## （8）baseline和benchmark是什么

1. 想强调一下，baseline 就只是「参照物」的意思，至于 baseline 系统是怎么来的、性能如何，并没有一定的标准。

比如：

- 如果你是机器学习的初学者，在做课程作业，那么你可能用「随机猜测」作为 baseline；如果你是要在顶会发论文，那么很可能就需要用当前最好的系统（称为 state of the art）来作 baseline，否则审稿人就会质疑。
- 如果你的论文的论点是「我针对某系统作了改进，提升了性能」，那么 baseline 就应该是未改进的系统（相当于生物实验中的「对照组」），它与改进后的系统只有一处不同，这样才能下结论说你的改进就是提升性能的原因。如果你的论文的论点是「我提出的方法 A 比已有的方法 B 更好」，那么 baseline 就应该是方法 B，即使它跟方法 A 毫无关系。
- 当你选定了一个 baseline 系统后，如果你能联系上作者，索取到他的代码，就可以直接用作者的实现作为 baseline；如果联系不上，就只能自己复现。有时候，你选择的 baseline 是你要研究的更广阔的框架下的一个特例，而你自己实现了框架下的其它方法，此时为了让系统之间只有一处不同，你可能会主动选择在框架下重新实现 baseline。
- 如果你是参加比赛，那么主办方常常会主动提供 baseline 系统。你可以在它的基础上做修改，也可以另起炉灶重新实现自己的系统。

2. 一个算法之所以被称为benchmark，是因为它的性能已经被广泛研究，人们对它性能的表现形式、测量方法都非常熟悉，因此可以作为标准方法来衡量其他方法的好坏。
   这里需要区别state-of-the-art（SOTA），能够称为SOTA的算法表明其性能在当前属于最佳性能。如果一个新算法以SOTA作为benchmark，这当然是最好的了，但如果比不过SOTA，能比benchmark要好，且方法有一定创新，也是可以发表的。

简而言之，
benchmark一般是和同行中比较牛的算法比较，比牛算法还好，那你可以考虑发好一点的会议/期刊；
baseline一般是自己算法优化和调参过程中自己和自己比较，目标是越来越好，当性能超过benchmark时，可以发表了，当性能甚至超过SOTA时，恭喜你，考虑投顶会顶刊啦。

## （9）交叉熵损失讲解

https://www.cnblogs.com/aijianiula/p/9460842.html

![image-20221019201143145](C:\Users\lpl\AppData\Roaming\Typora\typora-user-images\image-20221019201143145.png)

![image-20221019201227306](C:\Users\lpl\AppData\Roaming\Typora\typora-user-images\image-20221019201227306.png)

![image-20221019201251437](C:\Users\lpl\AppData\Roaming\Typora\typora-user-images\image-20221019201251437.png)

![image-20221019201305931](C:\Users\lpl\AppData\Roaming\Typora\typora-user-images\image-20221019201305931.png)

#               三、迁移学习相关知识

## 3.1 迁移学习问题集锦

### （1）

## 3.2 零样本学习问题集锦

<img src="C:\Users\lpl\AppData\Roaming\Typora\typora-user-images\image-20220913112157105.png" alt="image-20220913112157105" style="zoom:50%;" />





# 三、数据分析相关知识

## 1. 特征选择

**关于数据挖掘的五大流程：①获取场景数据集；②数据预处理：从数据中检测，纠正或删除损坏，不准确或不适用于模型的记录的过程。**可能面对的问题有：数据类型不同，比如有的是文字，有的是数字，有的含时间序列，有的连续，有的间断。数据预处理的目的是让数据适应模型，匹配模型的需求。**③特征工程：将原始数据转换为更能代表预测模型的潜在问题的特征的过程，可以通过挑选最相关的特征，提取特征以及创造特征来实现，其中创造特征又经常以降维算法的方式实现。**可能面对的问题有：特征之间有相关性，特征和标签无关，特征太多或太小，或者干脆就无法表现出应有的数据现象或无法展示数据的真实面貌。**特征工程的目的：1）降低计算成本；2）提升模型上线。④建模：测试模型并预测出结果；⑤上线：验证模型效果。**

**在机器学习中，如果算法只能处理数值数据，因此需要将文字等转换成数据表现。因此，出现了编码与哑变量（OrdinalEncoder, OneHotEncoder）方式。**

对于SVM、PCA、Kmeans都已单独写了文章。

**特征选择主要有：Filter过滤法（方差过滤、相关性过滤）；Embedded嵌入法；Wrapper包装法；降维算法**。

在业务过程中，可能会遇到很多特征，这些特征不能仅限于依赖业务的理解来进行选择，因此可以选择“特征选择”方法选择特征。

**1. Filter过滤法**

过滤方法通常用作预处理步骤，特征选择完全独立于任何机器学习算法，它是根据各种统计检验中的分数以及相关性的各项指标来选择特征。

![img](https://img-blog.csdnimg.cn/e3ee7d5fe07043a58ca19b0324bbe237.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA6L2p6KW_6ZOO,size_20,color_FFFFFF,t_70,g_se,x_16)![点击并拖拽以移动](data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==)

 **①方差过滤**

这是通过特征本身的方差来筛选特征的类。比如一个特征本身的方差很小，就表示样本在这个特征上基本没有差异，可能特征中的大多数值都一样，甚至整个特征的取值都相同，那这个特征对于样本区分没有任何作用。**所以无论接下来的特征工程要做什么，都要优先消除方差为0的特征**。VaricanceThreshold有重要参数threshold，表示方差的阈值，表示舍弃所有方差小于threshold的特征，不填的时候默认为0，即删除所有记录都相同的特征。

![img](https://img-blog.csdnimg.cn/574e62b954914c77afb3465ae65c497b.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA6L2p6KW_6ZOO,size_20,color_FFFFFF,t_70,g_se,x_16)![点击并拖拽以移动](data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==)

 可以看见，我们已经删除了方差为0的特征，但是依然剩下了708多个特征，明显还需要进一步的特征选择。然而，如果我们知道我们需要多少个特征，方差也可以帮助我们将特征选择一步到位。比如说，**我们希望留下一半的特征，那可以设定一个让特征总数减半的方差阈值，只要找到特征方差的中位数，再将这个中位数作为参数threshold的值输入就好了**。

![img](https://img-blog.csdnimg.cn/42186bb9586b47cdb59b09641fdfe84c.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA6L2p6KW_6ZOO,size_20,color_FFFFFF,t_70,g_se,x_16)![点击并拖拽以移动](data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==)

 为什么随机森林运行如此之快？为什么方差过滤对随机森林没很大的有影响？这是由于两种算法的原理中涉及到的计算量不同。最近邻算法KNN，单棵决策树，支持向量机SVM，神经网络，回归算法，都需要遍历特征或升维来进行运算，所以他们本身的运算量就很大，需要的时间就很长，因此方差过滤这样的特征选择对他们来说就尤为重要。但对于不需要遍历特征的算法，比如随机森林，它随机选取特征进行分枝，本身运算就非常快速，因此特征选择对它来说效果平平。这其实很容易理解，无论过滤法如何降低特征的数量，随机森林也只会选取固定数量的特征来建模；而最近邻算法就不同了，特征越少，距离计算的维度就越少，模型明显会随着特征的减少变得轻量。因此，过滤法的主要对象是：需要遍历特征或升维的算法们，而过滤法的主要目的是：在维持算法表现的前提下，帮助算法们降低计算成本。

 **②相关性过滤**

**方差挑选完毕之后，就要考虑特征的相**关性。我们希望能够选出与标签相关且有意义的特征，因为这样的特征能够为我们提供大量的信息，在sklearn中，我们有三种常用的方法来评判特征与标签之间的相关性：**卡方，F检验， 互信息**。

卡方过滤：

卡方过滤是专门针对离散型标签（即分类问题）的相关性过滤。卡方检验类feature_selection.chi2计算每个非负特征和标签之间的卡方统计量，并按照卡方统计量由高到低为特征排名。

另外，如果卡方检验检测到某个特征中所有的值都相同，会提示我们使用方差先进行方差过滤。

![img](https://img-blog.csdnimg.cn/f76fd9bed98a4b4394a059778c994d46.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA6L2p6KW_6ZOO,size_20,color_FFFFFF,t_70,g_se,x_16)![点击并拖拽以移动](data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==)

 F检验：

![img](https://img-blog.csdnimg.cn/6a49baf6c843429ca036a5d62bda8e14.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA6L2p6KW_6ZOO,size_20,color_FFFFFF,t_70,g_se,x_16)![点击并拖拽以移动](data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==)

 ![img](https://img-blog.csdnimg.cn/603683f22b4d437f8c4a2cce6283ec94.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA6L2p6KW_6ZOO,size_20,color_FFFFFF,t_70,g_se,x_16)![点击并拖拽以移动](data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==)

 互信息法：

![img](https://img-blog.csdnimg.cn/3e308b22a879471d986896c00e4c5247.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA6L2p6KW_6ZOO,size_20,color_FFFFFF,t_70,g_se,x_16)![点击并拖拽以移动](data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==)

 **通常来说，我会建议，先使用方差过滤，然后使用互信息法来捕捉相关性。**

**2. Embedded嵌入法**

![img](https://img-blog.csdnimg.cn/37a1f2f5798344f6a8240022bc6c21d5.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA6L2p6KW_6ZOO,size_20,color_FFFFFF,t_70,g_se,x_16)![点击并拖拽以移动](data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==)

 **3. Wrapper包装法**

![img](https://img-blog.csdnimg.cn/2e58eeed38124a3ab4fa0cb28c0a3229.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA6L2p6KW_6ZOO,size_20,color_FFFFFF,t_70,g_se,x_16)![点击并拖拽以移动](data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==)

  ![img](https://img-blog.csdnimg.cn/fa9fb9e770cd411ab72fb3c6857c49f1.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA6L2p6KW_6ZOO,size_20,color_FFFFFF,t_70,g_se,x_16)![点击并拖拽以移动](data:image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==)

# 四、各种数学相关知识

## 4.1 直方图与柱状图

<img src="C:\Users\lpl\AppData\Roaming\Typora\typora-user-images\image-20220915211900505.png" alt="image-20220915211900505" style="zoom:50%;" />

<img src="C:\Users\lpl\AppData\Roaming\Typora\typora-user-images\image-20220915211914674.png" alt="image-20220915211914674" style="zoom:50%;" />

## 4.2 笛卡尔积

笛卡尔乘积是指在数学中，两个[集合](https://baike.baidu.com/item/集合?fromModule=lemma_inlink)*X*和*Y*的笛卡尔积（Cartesian product），又称[直积](https://baike.baidu.com/item/直积/6537064?fromModule=lemma_inlink)，表示为*X* × *Y*，第一个对象是*X*的成员而第二个对象是*Y*的所有可能[有序对](https://baike.baidu.com/item/有序对?fromModule=lemma_inlink)的其中一个成员。
$$
表达式：A×B = {(x,y)|x∈A∧y∈B}
$$






#             五、专业相关欠缺知识

## 5.1 时域转频域

### 时域：事件按照时间的先后顺序发生。顾名思义就是随着时间的推移，我们所能直观感受到的东西或事物。

横轴：时间

纵轴：震动幅度（音量的高低）

**采样频率**：（单位是HZ）是指将模拟声音波形进行数字化时，每秒钟抽取声波幅度样本的次数。通俗的讲采样频率是指计算机单位时间内能够采集多少个信号样本。

采样频率的选择应该遵循**奈奎斯特**（Harry Nyquist）采样理论：如果对某一模拟信号进行采样，则采样后可还原的最高信号频率只有采样频率的一半，或者说只要采样频率高于输入信号最高频率的两倍，就能从采样信号系列重构原始信号。**正常人听觉的频率范围大约在20Hz~20kHz之间**，**根据奈奎斯特采样理论，为了保证声音不失真，采样频率应该在40kHz左右**。常用的音频采样频率有8kHz、11.025kHz、22.05kHz、16kHz、37.8kHz、44.1kHz、48kHz等，如果采用更高的采样频率，还可以达到DVD的音质。

傅立叶告诉我们，任何周期函数都可以看作不同振幅，不同相位的正弦波的叠加。贯穿时域和频域的方法之一，就是傅立叶分析，傅立叶分析又分为两个部分：傅立叶级数和傅立叶变换。

<img src="C:\Users\lpl\AppData\Roaming\Typora\typora-user-images\image-20220927195847355.png" alt="image-20220927195847355" style="zoom:50%;" />

<img src="F:\微信保存文件\WeChat Files\lianpenglong\FileStorage\File\2022-10\Page1.jpg" alt="Page1" style="zoom: 25%;" />

<img src="C:\Users\lpl\AppData\Roaming\Typora\typora-user-images\image-20221010112448788.png" alt="image-20221010112448788" style="zoom:50%;" />

### 频域：

通过傅里叶变换可以把信号从时域转换到频域；（将时域的信号（信号可以是周期与非周期信号）变成频域形式并加以分析的方法称为频谱分析。其目的是把复杂的时域波形，经过某种变换分解为若干单一的谐波分量来研究，以获得信号的频率结构以及各谐波和相位信息。这某种变换可以是傅立叶级数，也可以是傅立叶变换进行变换，这两者目的都一样，都是把时域信号变成频域以便于信号分析）

之前一直不理解频域里的频谱取值是怎么来的，后来发现它是和时域里的采样率相对应的，单位都是HZ， 时域如果1s采样 16000HZ， 转换到频域的频率范围就是（0， 16000HZ），相当于这1s 内的波形可以由这 16000个不同的正玄波叠加而成。

**短时傅里叶变换（窗式傅里叶变换）**

如果把一段音频直接FFT，因为时间较长，不能有效的逼近时域信号，会使信号太过平滑，于是又有了短时傅里叶变换 stft，用窗口滑动进行 FFT, 比如20ms 一次，相邻之间可以有重叠;

基本思想：局部平稳化-把长的非平稳随机过程看成是一系列短时随机平稳信号的叠加，短时性可通过在时间上加窗口函数实现（即截取一部分源数据）。通过该方法，人们至少可以说，无论发现了什么频率成分，它一定是发生在信号被截取的某个特定时间段内。

可用的函数：librosa.stft( )，音频处理库 librosa 很强大，可输出各种频谱。



### 时域信号与频域信号的关系

无论是连续的还是非连续的，周期信号用傅立叶级数来表示，非周期信号用傅立叶变换来表示；

时域信号是连续非周期的，则傅立叶变换后频域信号是连续非周期的；

时域信号是连续周期的，则傅立叶级数变换后频域信号是离散非周期的；

时域信号是离散的非周期时间信号，则DTFT之后，其频谱是连续的周期函数；

时域信号是离散的周期时间信号，则DTFT之后，其频谱是离散的周期函数。

<img src="https://img-blog.csdnimg.cn/20200815163417485.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0NTU0OTY0,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" style="zoom:50%;" />

离散时间信号的傅立叶变换DTFT

像模拟信号一样，采样信号或数字信号序列也存在着傅里叶变换，通常称作离散时间信号的傅里叶变换，即DTFT；
对连续时间信号在时域内进行采样的结果是频域内频谱的周期延拓，也就是说采样序列的频谱是周期函数，它可以用傅里叶级数表示，傅里叶级数的稀疏就相当于采样序列，因此把一般序列x(n)的DTFT定义为：

DTFT的逆变换为：

<img src="https://img-blog.csdnimg.cn/20200815164248358.png#pic_center" alt="img" style="zoom:50%;" />

DTFT中的级数求和不一定总是收敛的，若x(n)绝对可和，则该级数绝对收敛（充分条件）

### 傅立叶变换、拉普拉斯变换、z变换的联系是什么？

傅里叶变换需要满足一个条件，即所谓的狄利克雷条件（要求信号绝对可积/绝对可和），为了始补，满足这一条件的信号，也能读出它的频率，拉普拉斯变换和Z变换，对频率的含义做出了扩充，使得大多数有用信号都具有了对应的“频率”域表达式，方便对各个器件的设计。

这里，我们并不是通过拉氏变换和Z变换获取不满足狄利克雷条件的函数的傅氏变换。事实上由于收敛域的问题，这些函数的傅氏变换是不收敛的，即使通过拉氏变换和Z变换也不可能获得这些函数的傅氏变换。
**拉氏变换和Z变换的意义，是将频率域的某些限制条件A，转化为复频率域中与之等价的相应条件A’，然后在复频域内直接观察信号或系统的拉氏变换或Z变换，看X(s)或X(z)是否满足条件A’，得到相应的结论。**用这个结论代替傅里叶变换的结论（因为傅里叶变换不存在，无法得出结论）。

他们之间的关系到底是什么？
首先，傅里叶变换粗略分来包括连续时间傅里叶变换（CTFT）、离散时间傅里叶变换（DTFT）。
**CTFT是将连续时间信号变换到频域，将频率的含义扩充之后，就得到拉普拉斯变换。**
**DTFT是将离散时间信号变换到频域，将频率的含义扩充之后，就得到Z变换。**

Z变换：
序列x(n)的z变换定义为:
<img src="https://img-blog.csdnimg.cn/20200815165450450.png#pic_center" alt="img" style="zoom:50%;" /> 

## 5.2 扭矩、力矩

![img](https://bkimg.cdn.bcebos.com/pic/738b4710b912c8fc53d1f052fa039245d688210a?x-bce-process=image/watermark,image_d2F0ZXIvYmFpa2U4MA==,g_7,xp_5,yp_5/format,f_auto)

力矩在物理学里是指作用力使物体绕着转动轴或支点转动的趋向。力矩，力对物体产生转动作用的物理量，可以分为力对轴的矩和力对点的矩。转动力矩又称为转矩或扭矩。

![img](https://pic2.zhimg.com/80/v2-12c65dbc570007bba456749cbab8c039_720w.jpg)

力矩的概念在我们日常生活中随处可见，从小时候玩过的跷跷板，到阿基米德的名人名言——“给我一个支点，我将撬动整个地球”，这些都体现着力矩的含义。同样，在汽车上力矩也是无处不在，只不过通过一系列的传动轴的旋转，这里的力矩称之为扭矩。扭矩的大小直接影响着动力输出的工作效率、能源消耗、甚至运转寿命及安全性能等等因素。

**力矩与转矩的区别：**

二者所涵盖的范围不同，力矩的范围更宽泛，一切力乘以力臂的结果都可以称之为力矩，但是转矩一般指旋转的物体所受到的力矩。举例来说，车轮旋转时，地面摩擦力与车轮半径的乘积一般称之为转矩，但是也是力矩的一种。而用瓶起子开啤酒瓶一般称之为力矩，而不能说是转矩。

**转矩与扭矩的区别：**

使机器元件转动（包括有转动倾向）的力偶或力矩叫转动力矩，简称转矩。任何元件在转矩的作用下，必定产生某种程度的扭转变形（可能包括弹性变形和塑性变形）。因此，习惯上又常把转动力矩叫扭转力矩，简称扭矩。二者可以在任何领域混用，但扭矩在工程技术上用的更普遍些。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 

**电机功率、转速、输出扭矩之间的关系推导**

![image-20220922173300427](C:\Users\lpl\AppData\Roaming\Typora\typora-user-images\image-20220922173300427.png)

## 5.3 采样频率

采样频率也称为采样速度或者采样率，定义了单位时间内从连续信号中提取并组成离散信号的采样个数，用Hz来表示。采样频率的倒数是采样周期或者叫采样时间，它是采样之间的时间间隔。通俗来讲，采样频率是指计算机单位时间内能够采集多少个信号样本。

对于信号采样的过程，我们需要遵循乃奎斯特采样定律，采样频率应大于信号最高频率的两倍。

## 5.4 异常检测与故障诊断的区别

异常检测就是寻找不符合期望行为的数据异常点或者离群点。在现实世界中有着广泛的应用场景，例如信用卡欺诈检测，保险欺诈检测，医疗健康辅助诊断，网络入侵检测，安全关键系统错误检测，军事侦察等。在异常检测中，由于异常点少之又少，大部分是正常样本，异常只是相对小概率事件，并且异常点的特征表现非常不集中，即异常种类非常多，千奇百怪，即正常的情况大同小异，而异常各不相同，这种情况用有限的正例样本（异常点）训练有监督模型学习就很难从中学到有效的规律，再加上异常数据往往是比较难获得的，所以异常检测往往用无监督学习或半监督学习方法来进行。

故障诊断主要是为了定位系统的故障，并判断出故障发生的位置，发生原因，以及帮助进行如何恢复的决策。应用故障诊断需要对诊断目标的机理比较了解，从而建立出模型，进而通过收集数据进行数据处理，分析，与诊断故障，工业上应用比较多的是用于诊断某个机械设备的故障。故障诊断发展的大方向是故障预测、PHM、孪生模型的故障诊断和预测等。

## 5.5 一次设备和 

一次设备是指直接用于生产、输送和分配电能的生产过程的高压电气设备。它包括发电机、变压器、断路器、隔离开关、自动开关、接触器、刀开关、母线、输电线路、电力电缆、电抗器、电动机等。二次设备是指对一次设备的工作进行监测、控制、调节、保护以及为运行、维护人员提供运行工况或生产指挥信号所需的低压电气设备。如熔断器、按钮、指示灯、控制开关、继电器、控制电缆、仪表、信号设备、自动装置等。
生产和转换电能的设备:如将机械能转换成电能的发电机、变换电压、传输电能的变压器，将电能变成机械能的电动机等。接通和断开电路的开关设备:如高低压断路器、负荷开关、熔断器、隔离开关、接触器、磁力启动器等。保护电气:如限制短路电流的电抗器、防御过电压的避雷器等。载流导体:如传输电能的软、硬导体及电缆等。

# 六、博士课题

## 6.1 数据集分析



## 6.2 模型分析

### 6.2.1 开放集模型



## 6.3 实验结果分析

### （1）多分类混淆矩阵

在多分类任务中，可用混淆矩阵来比较分类结果与实际测得值，从而直观地表示各类别的分类状态，多分类混淆矩阵如图所示。

<img src="C:\Users\lpl\AppData\Roaming\Typora\typora-user-images\image-20220922200021322.png" alt="image-20220922200021322" style="zoom: 67%;" />

混淆矩阵列代表预测值为某一个类别，行代表真实标签为某一个类别。对于某类别而言，可将用户按真实标签与预测标签分为真阳性(True Positive, TP)、假阳性(False Positive, FP)、真阴性(True Negative, TN)与假阴性(False Negative, FN)。TP表示将此类别用户正确地预测出来；FP表示将非此类别的用户错误地预测为此类别；TN表示将非此类别的用户正确地预测为非此类别；FN表示将此类别用户错误地预测为非此类别。 

以混淆矩阵为基础，可得分类器的多个评价指标，二分类中常用指标包括准确率(Accuracy, ACC)、精确度(Precision, PRE)、查全率 Recall和 F1分数(F1-Score, F1)。其中 ACC为所预测样本中预测正确的比例；PRE为所有预测为某类别的用户中实际为此类用户的占比；Recall为所有实际为某类别的用户中预测为此类用户的占比；F1为与 PRE和 Recall有关的综合指标。各指标计算式分别如下：

<img src="C:\Users\lpl\AppData\Roaming\Typora\typora-user-images\image-20220922200251728.png" alt="image-20220922200251728" style="zoom: 67%;" />

在计算多分类的评价指标时，需将 n分类拆分为 n个二分类进行计算，即可得到各个类别的评价指标结果。将各个类别的结果进行平均可得分类任务的整体指标，其中平均方式有两种，即宏平均(Macro average)与微平均(Micro average)。宏平均是将 n个二分类的各指标结果直接求算数平均值，微平均则为 n个二分类结果的 TP、FP、TN与 FN值对应相加，再根据各指标算式进行计算。**宏平均在计算的过程中不考虑各个类别的样本比例，因此不适合数据集不平衡的情况，所以本文选用微平均作为多分类结果的计算方式。**使用 microA、microP、microR、microF1分别表示多分类结果各类别准确率、精确度、召回率与 F1分数的平均值，各指标计算式与二分类时 ACC、PRE、Recall和 F的计算式形式相同，但变量含义不同，需将分类结果中每个类别的 TP、FP、TN与 FN样本数相加再进行计算。

 **多类别混淆矩阵的推导及程序****（例子）**

1. **假如有以下数据**

<img src="C:\Users\lpl\AppData\Roaming\Typora\typora-user-images\image-20220923212634707.png" alt="image-20220923212634707" style="zoom: 50%;" />

可以看出，上表为一份样本量为9，类别数为3的含标注结果的三分类预测样本。TN对于准召的计算而言是不需要的，因此下面的表格中未统计该值。

① 按照定义计算Precision、Recall
1. 对于类别A

<img src="C:\Users\lpl\AppData\Roaming\Typora\typora-user-images\image-20220923213315039.png" alt="image-20220923213315039" style="zoom:50%;" />

2. 对于类别B

<img src="C:\Users\lpl\AppData\Roaming\Typora\typora-user-images\image-20220923213416353.png" alt="image-20220923213416353" style="zoom:50%;" />

3. 对于类别C

<img src="C:\Users\lpl\AppData\Roaming\Typora\typora-user-images\image-20220923213447103.png" alt="image-20220923213447103" style="zoom:50%;" />

2. **调用sklearn的api进行验证**

```python
from sklearn.metrics import classification_report
from sklearn.metrics import precision_score, recall_score, f1_score

true_lable = [0, 0, 0, 0, 1, 1, 1, 2, 2] # 真实标签类别
prediction = [0, 0, 1, 2, 1, 1, 2, 1, 2]

measure_result = classification_report(true_lable, prediction)
print('measure_result = \n', measure_result)

#------------------打印结果
measure_result = 
               precision    recall  f1-score   support

           0       1.00      0.50      0.67         4
           1       0.50      0.67      0.57         3
           2       0.33      0.50      0.40         2

    accuracy                           0.56         9
   macro avg       0.61      0.56      0.55         9
weighted avg       0.69      0.56      0.58         9
```

3. **Micro-F1、Macro-F1、weighted-F1**

<img src="https://img-blog.csdnimg.cn/44e2b38ce0314309ba93450bec9eb849.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5b-N6ICF44Gu5Lmx5aSq6YOO,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述" style="zoom:50%;" />

总的来说，微观F1(micro-F1)和宏观F1(macro-F1)都是F1合并后的结果，这两个F1都是用在多分类任务中的评价指标，是两种不一样的求F1均值的方式；**micro-F1和macro-F1的计算方法有差异**，得出来的结果也略有差异；

**①Micro-F1**

Micro-F1 不需要区分类别，直接使用总体样本的准召计算f1 score。

> 计算方法：先计算所有类别的总的Precision和Recall，然后计算出来的F1值即为micro-F1；
>
> 使用场景：在计算公式中考虑到了每个类别的数量，所以适用于数据分布不平衡的情况；但同时因为考虑到数据的数量，所以在数据极度不平衡的情况下，数量较多数量的类会较大的影响到F1的值；

该样本的混淆矩阵如下：

<img src="C:\Users\lpl\AppData\Roaming\Typora\typora-user-images\image-20220923214425420.png" alt="image-20220923214425420" style="zoom:50%;" />

<img src="C:\Users\lpl\AppData\Roaming\Typora\typora-user-images\image-20220923214458277.png" alt="image-20220923214458277" style="zoom:50%;" />

**②Macro-F1**

不同于micro f1，macro f1需要先计算出每一个类别的准召及其f1 score，然后通过求均值得到在整个样本上的f1 score。

> - 计算方法：将所有类别的Precision和Recall求平均，然后计算F1值作为macro-F1；
> - 使用场景：没有考虑到数据的数量，所以会平等的看待每一类（因为每一类的precision和recall都在0-1之间），会相对受高precision和高recall类的影响较大；

<img src="C:\Users\lpl\AppData\Roaming\Typora\typora-user-images\image-20220923214645655.png" alt="image-20220923214645655" style="zoom:50%;" />

**③weighted-F1**

除了micro-F1和macro-F1，还有weighted-F1，是一个将F1-score乘以该类的比例之后相加的结果，也可以看做是macro-F1的变体吧。

weighted-F1和macro-F1的区别在于：macro-F1对每一类都赋予了相同的权重，而weighted-F1则根据每一类的比例分别赋予不同的权重。

4. **指标的选择问题**

“我们看到，对于 Macro 来说， 小类别相当程度上拉高了 Precision 的值，而实际上， 并没有那么多样本被正确分类，考虑到实际的环境中，真实样本分布和训练样本分布相同的情况下，这种指标明显是有问题的， 小类别起到的作用太大，以至于大样本的分类情况不佳。 而对于 Micro 来说，其考虑到了这种样本不均衡的问题， 因此在这种情况下相对较佳。

总的来说， **如果你的类别比较均衡，则随便； 如果你认为大样本的类别应该占据更重要的位置， 使用Micro； 如果你认为小样本也应该占据重要的位置，则使用 Macro**； 如果 Micro << Macro ， 则意味着在大样本类别中出现了严重的分类错误； 如果 Macro << Micro ， 则意味着小样本类别中出现了严重的分类错误。

**为了解决 Macro 无法衡量样本均衡问题，一个很好的方法是求加权的 Macro， 因此 Weighed F1 出现了**。”

5. **代码问题**

```python
#------------------------------
#--------数据01--------------
#---------------------------
true_lable = [0, 0, 0, 0, 1, 1, 1, 2, 2]
prediction = [0, 0, 1, 2, 1, 1, 2, 1, 2]

from sklearn.metrics import classification_report
from sklearn.metrics import precision_score, recall_score, f1_score

true_lable = [0, 0, 0, 0, 1, 1, 1, 2, 2]
prediction = [0, 0, 1, 2, 1, 1, 2, 1, 2]


measure_result = classification_report(true_lable, prediction)
print('measure_result = \n', measure_result)

print("----------------------------- precision（精确率）-----------------------------")
precision_score_average_None = precision_score(true_lable, prediction, average=None)
precision_score_average_micro = precision_score(true_lable, prediction, average='micro')
precision_score_average_macro = precision_score(true_lable, prediction, average='macro')
precision_score_average_weighted = precision_score(true_lable, prediction, average='weighted')
print('precision_score_average_None = ', precision_score_average_None)
print('precision_score_average_micro = ', precision_score_average_micro)
print('precision_score_average_macro = ', precision_score_average_macro)
print('precision_score_average_weighted = ', precision_score_average_weighted)

print("\n\n----------------------------- recall（召回率）-----------------------------")
recall_score_average_None = recall_score(true_lable, prediction, average=None)
recall_score_average_micro = recall_score(true_lable, prediction, average='micro')
recall_score_average_macro = recall_score(true_lable, prediction, average='macro')
recall_score_average_weighted = recall_score(true_lable, prediction, average='weighted')
print('recall_score_average_None = ', recall_score_average_None)
print('recall_score_average_micro = ', recall_score_average_micro)
print('recall_score_average_macro = ', recall_score_average_macro)
print('recall_score_average_weighted = ', recall_score_average_weighted)

print("\n\n----------------------------- F1-value-----------------------------")
f1_score_average_None = f1_score(true_lable, prediction, average=None)
f1_score_average_micro = f1_score(true_lable, prediction, average='micro')
f1_score_average_macro = f1_score(true_lable, prediction, average='macro')
f1_score_average_weighted = f1_score(true_lable, prediction, average='weighted')
print('f1_score_average_None = ', f1_score_average_None)
print('f1_score_average_micro = ', f1_score_average_micro)
print('f1_score_average_macro = ', f1_score_average_macro)
print('f1_score_average_weighted = ', f1_score_average_weighted)

#---------------打印结果
measure_result = 
               precision    recall  f1-score   support

           0       1.00      0.50      0.67         4
           1       0.50      0.67      0.57         3
           2       0.33      0.50      0.40         2

    accuracy                           0.56         9
   macro avg       0.61      0.56      0.55         9
weighted avg       0.69      0.56      0.58         9

----------------------------- precision（精确率）-----------------------------
precision_score_average_None =  [1.         0.5        0.33333333]
precision_score_average_micro =  0.5555555555555556
precision_score_average_macro =  0.611111111111111
precision_score_average_weighted =  0.6851851851851852


----------------------------- recall（召回率）-----------------------------
recall_score_average_None =  [0.5        0.66666667 0.5       ]
recall_score_average_micro =  0.5555555555555556
recall_score_average_macro =  0.5555555555555555
recall_score_average_weighted =  0.5555555555555556


----------------------------- F1-value-----------------------------
f1_score_average_None =  [0.66666667 0.57142857 0.4       ]
f1_score_average_micro =  0.5555555555555556
f1_score_average_macro =  0.546031746031746
f1_score_average_weighted =  0.5756613756613757

Process finished with exit code 0


#------------------------------
#--------数据02--------------
#---------------------------
true_lable = [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3]
prediction = [3, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 1, 1, 1, 1, 1, 1, 3, 1, 2, 2, 2, 2, 2, 3, 0, 3, 3, 3, 3]

from sklearn.metrics import classification_report
from sklearn.metrics import precision_score, recall_score, f1_score

true_lable = [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3]
prediction = [3, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 1, 1, 1, 1, 1, 1, 3, 1, 2, 2, 2, 2, 2, 3, 0, 3, 3, 3, 3]

measure_result = classification_report(true_lable, prediction)
print('measure_result = \n', measure_result)

print("----------------------------- precision（精确率）-----------------------------")
precision_score_average_None = precision_score(true_lable, prediction, average=None)
precision_score_average_micro = precision_score(true_lable, prediction, average='micro')
precision_score_average_macro = precision_score(true_lable, prediction, average='macro')
precision_score_average_weighted = precision_score(true_lable, prediction, average='weighted')
print('precision_score_average_None = ', precision_score_average_None)
print('precision_score_average_micro = ', precision_score_average_micro)
print('precision_score_average_macro = ', precision_score_average_macro)
print('precision_score_average_weighted = ', precision_score_average_weighted)

print("\n\n----------------------------- recall（召回率）-----------------------------")
recall_score_average_None = recall_score(true_lable, prediction, average=None)
recall_score_average_micro = recall_score(true_lable, prediction, average='micro')
recall_score_average_macro = recall_score(true_lable, prediction, average='macro')
recall_score_average_weighted = recall_score(true_lable, prediction, average='weighted')
print('recall_score_average_None = ', recall_score_average_None)
print('recall_score_average_micro = ', recall_score_average_micro)
print('recall_score_average_macro = ', recall_score_average_macro)
print('recall_score_average_weighted = ', recall_score_average_weighted)

print("\n\n----------------------------- F1-value-----------------------------")
f1_score_average_None = f1_score(true_lable, prediction, average=None)
f1_score_average_micro = f1_score(true_lable, prediction, average='micro')
f1_score_average_macro = f1_score(true_lable, prediction, average='macro')
f1_score_average_weighted = f1_score(true_lable, prediction, average='weighted')
print('f1_score_average_None = ', f1_score_average_None)
print('f1_score_average_micro = ', f1_score_average_micro)
print('f1_score_average_macro = ', f1_score_average_macro)
print('f1_score_average_weighted = ', f1_score_average_weighted)

#-------------打印结果------
measure_result = 
               precision    recall  f1-score   support

           0       0.88      0.78      0.82         9
           1       0.86      0.75      0.80         8
           2       0.83      0.71      0.77         7
           3       0.56      0.83      0.67         6

    accuracy                           0.77        30
   macro avg       0.78      0.77      0.76        30
weighted avg       0.80      0.77      0.77        30

----------------------------- precision（精确率）-----------------------------
precision_score_average_None =  [0.875      0.85714286 0.83333333 0.55555556]
precision_score_average_micro =  0.7666666666666667
precision_score_average_macro =  0.7802579365079365
precision_score_average_weighted =  0.7966269841269841


----------------------------- recall（召回率）-----------------------------
recall_score_average_None =  [0.77777778 0.75       0.71428571 0.83333333]
recall_score_average_micro =  0.7666666666666667
recall_score_average_macro =  0.7688492063492064
recall_score_average_weighted =  0.7666666666666667


----------------------------- F1-value-----------------------------
f1_score_average_None =  [0.82352941 0.8        0.76923077 0.66666667]
f1_score_average_micro =  0.7666666666666667
f1_score_average_macro =  0.7648567119155354
f1_score_average_weighted =  0.7732126696832579

Process finished with exit code 0
```

### **多分类混淆矩阵画图**

图中纵轴是truth label，横轴是predicted label，那么第一行第一个0.6的含义是：模型分类正确的精度，每一行的和为100%。

### 多分类混淆矩阵最终的程序分析

<img src="F:\微信保存文件\WeChat Files\lianpenglong\FileStorage\File\2022-09\Page1(1).jpg" alt="Page1(1)" style="zoom: 28%;" />

<img src="C:\Users\lpl\AppData\Roaming\Typora\typora-user-images\image-20220927212734170.png" alt="image-20220927212734170" style="zoom: 50%;" />

<img src="C:\Users\lpl\AppData\Roaming\Typora\typora-user-images\image-20220927212802966.png" alt="image-20220927212802966" style="zoom: 50%;" />

<img src="C:\Users\lpl\AppData\Roaming\Typora\typora-user-images\image-20220927212817476.png" alt="image-20220927212817476" style="zoom: 50%;" />



## 6.4





# 七、学术软件使用

## 7.1 git使用

### （1）教程链接

https://www.bilibili.com/video/BV1Cr4y1J7iQ/?is_story_h5=false&p=1&share_from=ugc&share_medium=iphone&share_plat=ios&share_session_id=1E17EF54-5D8D-49B4-9FC5-BA9794F7CC23&share_source=WEIXIN&share_tag=s_i&timestamp=1663993522&unique_k=KHo38Nd&vd_source=96bd7e9a5befbf7a97d9f87e43c136de

Git是一个运行在你电脑上的版本控制软件，而Github是基于Git这个版本控制软件打造的网战。

#### 如何一步步安装及使用

具体参考：https://www.runoob.com/git/git-remote-repo.html

（1）git网站下载git客户端（如果没有github账户的去github网站注册）

https://git-scm.com/

（2）由于本地Git仓库和Github仓库之间的传输是通过SSH加密的，所以我们需要配置验证信息:

```
$ ssh-keygen -t rsa -C "penglonglian@foxmail.com" # 邮箱需要修改为注册Github时的邮箱
# 之后会要求确认路径和输入密码，我们这使用默认的一路回车就行，成功的话会在C:\Users\lpl\.ssh中生成.ssh文件夹，进去打开id_rsa.pub，复制里面的pub（全部复制即可）
# 回到github上，进入Account => Settings （账户配置），左边选择 SSH and GPG keys，然后点击 New SSH key 按钮,title 设置标题，可以随便填，粘贴在你电脑上生成的 key。
# 添加成功后界面会显示自己配置成功
# 为了验证是否成功，可以输入以下命令
$ ssh -T git@github.com
The authenticity of host 'github.com (52.74.223.119)' can't be established.
RSA key fingerprint is SHA256:nThbg6kXUpJWGl7E1IGOCspRomTxdCARLviKw6E5SY8.
Are you sure you want to continue connecting (yes/no/[fingerprint])? yes                   # 输入 yes
Warning: Permanently added 'github.com,52.74.223.119' (RSA) to the list of known hosts.
Hi tianqixin! You've successfully authenticated, but GitHub does not provide shell access. # 成功信息
# 以上命令说明我们已经成功连上了Github
# 之后就可以新建一个repository
# 创建之后就会在Quick setup-if you've done this kind of thing before下有一个SSH的类似于链接的东西（这个东西在提交的时候要用到）
```

```
# 要开始将自己文件夹下的.py等文件提交到自己的github了
$ git init
$ git add .
$ git commit -m "XXXX"

# 提交到github上
$ git remote add origin xxxxxxxxxxxxxxxx(创建的repository后产生的SSH类似于链接的东西)
$ git push -u origin master
```



### 2）使用过程中注意的命令

① 下载完的配置

```
git config --global user.name PancrasLPL
git config --global user.email lianpenglong@163.com
```

②两种命令

```
git clone   # 如果是从github上下载源码，可以使用git clone
# 比如想要让github上的文件下载到某个文件夹里/桌面上，就直接在相应的位置邮件打开Git bash here，然后git clone http...(github上的链接)
# 下载完之后会文件夹里会有一个.git文件，这个文件不需要操作
```

```
# 如果是自己的文件需要管理的时候，则需要新建一个文件夹，然后告诉git，帮我们管理它。
# 在自己想要管理的文件夹下右键Git bash here
# 然后输入git init, 相当于初始化
git init
# 接下来文件夹里立马创建了一个名为“.git”的隐藏文件夹，用来管理即将新建的源代码
# 除.git外的地方叫做工作区

# 接下来我们就需要将我们的文件提交给git仓库，即利用.git去帮助我们管理；但是不能直接操作.git文件
# 提交时有以下命令
git add . # .代表文件夹下所有的文件
git add main.py # 代表只提交main.py
# 上面的.代表当前文件夹的意思；表示将当前文件夹下的非空文件设置为准备提交的状态（子弹上膛，准备发射）
# 接下来就是提交
git commit -m "xxxx" # "xxx"这里面的内容是对本次提交的备注，eg."功能1已完成"

git log # 可以用git log来查看提交的历史记录
# 显示出来的一串东西其实就类似于身份证号
```

```
# 如果commit的文件有误，需要从上一次提交的进行恢复
git checkout HEAD xxx.py
# 如果把代码跑的乱七八糟的，也可以采用这种方式进行恢复
```

![image-20220929174348198](C:\Users\lpl\AppData\Roaming\Typora\typora-user-images\image-20220929174348198.png)

### （3）使用过程中出现的问题：

①：git clone出现 fatal: unable to access ‘https://github.com/...‘的两种解决方法

https://blog.csdn.net/weixin_45317091/article/details/113409909 （方法一）

②：OpenSSL SSL_read: Connection was reset, errno 10054的解决方法

https://blog.csdn.net/wjh1840226173/article/details/124355167

③：git commit提示nothing to commit, working tree clean

https://blog.csdn.net/milk_tiger/article/details/108546583

## 7.2 bibtex格式

是一种参考文献格式，纯文本的，你可以用记事本打开，里面的内容类似于Json一样的，一堆就是一个文献，等号前面是键，后面是值。很多文献管理软件打开这种格式后，想个数据库一样。